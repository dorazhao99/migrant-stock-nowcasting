{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MAPE(y_true, y_pred): \n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_all_models(X_train, X_test, y_train, y_test, preds = False):\n",
    "        \n",
    "    if X_train.shape[1] > 20:\n",
    "        X_train = X_train.values\n",
    "        X_train[:,22:] = np.log(X_train[:,22:])\n",
    "        X_test = X_test.values\n",
    "        X_test[:,22:] = np.log(X_test[:, 22:])\n",
    "    else:\n",
    "        X_train = np.log(X_train)\n",
    "        X_test = np.log(X_test)\n",
    "    y_train = np.log(y_train)\n",
    "        \n",
    "    # linear regression\n",
    "    lin_reg = LinearRegression()\n",
    "    lin_reg.fit(X_train, y_train)\n",
    "    lin_y_pred = np.exp(lin_reg.predict(X_test))\n",
    "    lin_mape = MAPE(y_test, lin_y_pred)\n",
    "    \n",
    "    \n",
    "    # random forest\n",
    "    rf_regr = RandomForestRegressor(max_depth=10, random_state=random, criterion=\"mae\")\n",
    "    rf_regr.fit(X_train, y_train)\n",
    "    rf_y_pred = np.exp(rf_regr.predict(X_test))\n",
    "    rf_mape = MAPE(y_test, rf_y_pred)\n",
    "    #print('Random Forests MAPE: {0}'.format(rf_mape))\n",
    "\n",
    "    '''\n",
    "    # adaboost \n",
    "    ada_regr = AdaBoostRegressor(random_state=random)\n",
    "    ada_regr.fit(X_train, y_train)\n",
    "    ada_y_pred = ada_regr.predict(X_test)\n",
    "    ada_mape = MAPE(y_test, ada_y_pred)\n",
    "    #print('AdaBoost MAPE: {0}'.format(MAPE(y_test, y_pred)))\n",
    "    '''\n",
    "    \n",
    "    # xgboost\n",
    "    xg_regr = XGBRegressor(max_depth=10, learning_rate=0.01, n_estimators=300, gamma=1, random_state=random)\n",
    "    xg_regr.fit(X_train, y_train)\n",
    "    xg_y_pred = np.exp(xg_regr.predict(X_test))\n",
    "    xg_mape = MAPE(y_test, xg_y_pred)\n",
    "    #print('XGBoost MAPE: {0}'.format(MAPE(y_test, y_pred)))\n",
    "    \n",
    "\n",
    "    #print('Linear Regression MAPE: {0}'.format(MAPE(y_test, y_pred)))\n",
    "    \n",
    "    if preds: return np.array([rf_mape, xg_mape, lin_mape]), [rf_y_pred, xg_y_pred, lin_y_pred]\n",
    "    \n",
    "    return np.array([rf_mape, xg_mape, lin_mape])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "_iso_splits = None\n",
    "def gen_splits(fb_df, variables = [], data_used = False):\n",
    "    global _iso_splits\n",
    "    if data_used:\n",
    "        variables.extend(['data_used_B', 'data_used_C', 'data_used_I', 'data_used_R', 'data_used_BR', 'data_used_CR', 'data_used_IR', 'data_used_CB', 'data_used_CBR'])\n",
    "        \n",
    "    total_columns = [col for col in fb_df.columns[2:] if '2019' not in col and '2020' not in col and ('un' in col or 'lvl' in col)]\n",
    "    predictors = fb_df[variables].values if variables != []\\\n",
    "        else fb_df[total_columns].values\n",
    "    gt = fb_df['migrant_pop_2019'].values\n",
    "    \n",
    "    high_predictors = predictors[fb_df['development_lvl'] == 1]\n",
    "    high_gt = gt[fb_df['development_lvl'] == 1]\n",
    "    low_predictors = predictors[fb_df['development_lvl'] == 0]\n",
    "    low_gt = gt[fb_df['development_lvl'] == 0]\n",
    "    \n",
    "    \n",
    "    if _iso_splits is None:\n",
    "        kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        _iso_splits = dict()\n",
    "        _iso_splits[\"all\"] = list(kf.split(predictors, gt))\n",
    "        _iso_splits[\"high\"] = list(kf.split(high_predictors, high_gt))\n",
    "        _iso_splits[\"low\"] = list(kf.split(low_predictors, low_gt))\n",
    "\n",
    "    splits = dict()\n",
    "    # 1. Randomly sample from all countries for training and test sets.\n",
    "    #splits[\"random_all\"] = train_test_split(predictors, gt, test_size=0.2, random_state=42)\n",
    "    splits[\"random_all\"] = [(predictors[i], predictors[j], gt[i], gt[j]) for i, j in _iso_splits[\"all\"]]\n",
    "    # 2. Train more developed, test less developed\n",
    "    # splits[\"train_high_test_low\"] = [(high_predictors, low_predictors, high_gt, low_gt)]\n",
    "    # 3. Train less developed, test more developed\n",
    "    # splits[\"train_low_test_high\"] = [(low_predictors, high_predictors, low_gt, high_gt)]\n",
    "    # 4. Randomly sample high for train+test\n",
    "    #splits[\"train_test_high\"] = train_test_split(high_predictors, high_gt, test_size=0.2, random_state=42)\n",
    "    splits[\"train_test_high\"] = [(high_predictors[i], high_predictors[j], high_gt[i], high_gt[j]) for i, j in _iso_splits[\"high\"]]\n",
    "    # 5. Randomly sample low for train+test\n",
    "    #splits[\"train_test_low\"] = train_test_split(low_predictors, low_gt, test_size=0.2, random_state=42)\n",
    "    splits[\"train_test_low\"] = [(low_predictors[i], low_predictors[j], low_gt[i], low_gt[j]) for i, j in _iso_splits[\"low\"]]\n",
    "\n",
    "    return splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in UN data\n",
    "un_df = pd.read_csv('../data/UN_data_clean.csv')\n",
    "# ground truth data for all of the models\n",
    "y = np.array((un_df[(un_df['age_group'] == 'Total') & (un_df['sex'] == 'both sexes') & (un_df['year'] == 2019)]\\\n",
    "          ['migrant_pop']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in combined fb_un_data\n",
    "#fb_df = pd.read_csv('../data/facebook_un_combined_2020.csv')\n",
    "#fb_df_2020 = pd.read_csv('../data/facebook_un_combined_2020.csv')\n",
    "#predictors = fb_df.values [:, 2:176]\n",
    "#gt = fb_df['migrant_2019'].values\n",
    "\n",
    "fb_df = pd.read_csv('../data/FB_UN_totals.csv')\n",
    "gt = fb_df['migrant_pop_2019'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb_un_age_sex = pd.read_csv('../data/FB_UN_age_sex.csv')\n",
    "fb_un_age_sex = fb_un_age_sex.drop('country_name', axis=1)\n",
    "fb_un_age_sex['oecd_member'] = fb_un_age_sex['oecd_member'].astype(int)\n",
    "\n",
    "fb_un_age_sex['data_used_B'] = ['B' in s for s in fb_un_age_sex['data_used']]\n",
    "fb_un_age_sex['data_used_C'] = ['C' in s for s in fb_un_age_sex['data_used']]\n",
    "fb_un_age_sex['data_used_I'] = ['I' in s for s in fb_un_age_sex['data_used']]\n",
    "fb_un_age_sex['data_used_R'] = ['R' in s for s in fb_un_age_sex['data_used']]\n",
    "fb_un_age_sex['data_used_BR'] = ['B' in s and 'R' in s for s in fb_un_age_sex['data_used']]\n",
    "fb_un_age_sex['data_used_CR'] = ['C' in s and 'R' in s for s in fb_un_age_sex['data_used']]\n",
    "fb_un_age_sex['data_used_IR'] = ['I' in s and 'R' in s for s in fb_un_age_sex['data_used']]\n",
    "fb_un_age_sex['data_used_CB'] = ['C' in s and 'B' in s for s in fb_un_age_sex['data_used']]\n",
    "fb_un_age_sex['data_used_CBR'] = ['C' in s and 'B' in s and 'R' in s for s in fb_un_age_sex['data_used']]\n",
    "\n",
    "fb_un_age_sex = fb_un_age_sex.drop('data_used', axis=1)\n",
    "\n",
    "fb_un_age_sex['un_development_lvl'] = [0 if 'Less' in s else 1 for s in fb_un_age_sex['un_development_lvl']]\n",
    "\n",
    "fb_un_age_sex.loc[fb_un_age_sex['fb_penetration'] > 1, 'fb_penetration'] = 1\n",
    "\n",
    "fb_un_age_sex = pd.get_dummies(fb_un_age_sex)\n",
    "\n",
    "fb_un_age_sex['fb_expats_normalized'] = fb_un_age_sex['fb_expats'] / fb_un_age_sex['fb_penetration']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "depth = 5\n",
    "random = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = open('model_mapes_log_log.csv','w')\n",
    "writer = csv.writer(f, delimiter=',')\n",
    "writer.writerow([\"model\", \"split\", \"rf_mape\", \"xgboost_mape\", \"linreg_mape\"])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "rf = RandomForestRegressor()\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
    "\n",
    "splits = gen_splits(fb_df_2020)\n",
    "X_train, X_test, y_train, y_test = splits[\"random_all\"][0]\n",
    "rf_random.fit(X_train, y_train)\n",
    "\n",
    "rf_random.best_params_"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "best_random = rf_random.best_estimator_\n",
    "MAPE(y_test, best_random.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Autoregressive Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random_all: \n",
      "[15.33323484 44.21512607 11.29510699]\n",
      "\n",
      "\n",
      "train_test_high: \n",
      "[16.15800084 53.67463615  7.3213885 ]\n",
      "\n",
      "\n",
      "train_test_low: \n",
      "[20.11027827 43.07625938 11.85701237]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "splits = gen_splits(fb_df, ['migrant_pop_2015', 'migrant_pop_2017'])\n",
    "\n",
    "for (k, folds) in splits.items():\n",
    "    print(k + \": \")\n",
    "    mapes = np.array([0.]*3)\n",
    "    for fold in folds: \n",
    "        mapes+=run_all_models(*fold)\n",
    "    mapes/=len(folds)\n",
    "    #writer.writerow([\"autoregressive_baseline\", k, *mapes])\n",
    "    print(mapes)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "splits = gen_splits(fb_df, ['un_expat_total_age16_2017', 'un_expat_total_age16_2015'], data_used = True)\n",
    "\n",
    "for (k, folds) in splits.items():\n",
    "    print(k + \": \")\n",
    "    mapes = np.array([0.]*3)\n",
    "    for fold in folds: \n",
    "        mapes+=run_all_models(*fold)\n",
    "    mapes/=len(folds)\n",
    "    writer.writerow([\"autoregressive_baseline_data_used\", k, *mapes])\n",
    "    print(mapes)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Facebook Naive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random_all: \n",
      "[194.5915372  104.36927258 149.69018252]\n",
      "\n",
      "\n",
      "train_test_high: \n",
      "[59.44391981 57.60394614 50.11232441]\n",
      "\n",
      "\n",
      "train_test_low: \n",
      "[231.22202933 108.44051089 152.4569364 ]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "splits = gen_splits(fb_df, ['fb_expats'])\n",
    "\n",
    "for (k, folds) in splits.items():\n",
    "    print(k + \": \")\n",
    "    mapes = mapes = np.array([0.]*3)\n",
    "    for fold in folds: \n",
    "        mapes+=run_all_models(*fold)\n",
    "    mapes/=len(folds)\n",
    "    #writer.writerow([\"fb_naive\", k, *mapes])\n",
    "    print(mapes)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "splits = gen_splits(fb_df, ['total_expat'], True)\n",
    "\n",
    "for (k, folds) in splits.items():\n",
    "    print(k + \": \")\n",
    "    mapes = mapes = np.array([0.]*3)\n",
    "    for fold in folds: \n",
    "        mapes+=run_all_models(*fold)\n",
    "    mapes/=len(folds)\n",
    "    writer.writerow([\"fb_naive_data_used\", k, *mapes])\n",
    "    print(mapes)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoregressive + Facebook Expats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random_all: \n",
      "[16.11719022 44.44589401 13.16156887]\n",
      "\n",
      "\n",
      "train_test_high: \n",
      "[16.95024599 53.97661257  7.1113205 ]\n",
      "\n",
      "\n",
      "train_test_low: \n",
      "[19.53510405 42.62402311 14.96718015]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "splits = gen_splits(fb_df, ['migrant_pop_2015', 'migrant_pop_2017', 'fb_expats'])\n",
    "\n",
    "for (k, folds) in splits.items():\n",
    "    print(k + \": \")\n",
    "    mapes = mapes = np.array([0.]*3)\n",
    "    for fold in folds: \n",
    "        mapes+=run_all_models(*fold)\n",
    "    mapes/=len(folds)\n",
    "    #writer.writerow([\"autoregressive_plus_fb\", k, *mapes])\n",
    "    print(mapes)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "splits = gen_splits(fb_df, ['un_expat_total_age16_2017', 'un_expat_total_age16_2015', 'total_expat'], True)\n",
    "\n",
    "for (k, folds) in splits.items():\n",
    "    print(k + \": \")\n",
    "    mapes = mapes = np.array([0.]*3)\n",
    "    for fold in folds: \n",
    "        mapes+=run_all_models(*fold)\n",
    "    mapes/=len(folds)\n",
    "    writer.writerow([\"autoregressive_plus_fb_data_used\", k, *mapes])\n",
    "    print(mapes)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Facebook age-sex corrected"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "norm_columns = [col for col in fb_df.columns if 'normalized' in col]\n",
    "splits = gen_splits(fb_df, norm_columns)\n",
    "\n",
    "for (k, folds) in splits.items():\n",
    "    print(k + \": \")\n",
    "    mapes = mapes = np.array([0.]*3)\n",
    "    for fold in folds: \n",
    "        mapes+=run_all_models(*fold)\n",
    "    mapes/=len(folds)\n",
    "    writer.writerow([\"fb_age_sex_normalized\", k, *mapes])\n",
    "    print(mapes)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Facebook age-sex corrected (2020)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "norm_columns = [col for col in fb_df.columns if 'normalized' in col]\n",
    "splits = gen_splits(fb_df, norm_columns)\n",
    "\n",
    "for (k, folds) in splits.items():\n",
    "    print(k + \": \")\n",
    "    mapes = mapes = np.array([0.]*3)\n",
    "    for fold in folds: \n",
    "        mapes+=run_all_models(*fold)\n",
    "    mapes/=len(folds)\n",
    "    writer.writerow([\"fb_age_sex_normalized\", k, *mapes])\n",
    "    print(mapes)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "norm_columns = [col for col in fb_df.columns if 'normalized' in col]\n",
    "splits = gen_splits(fb_df, norm_columns, True)\n",
    "\n",
    "for (k, folds) in splits.items():\n",
    "    print(k + \": \")\n",
    "    mapes = mapes = np.array([0.]*3)\n",
    "    for fold in folds: \n",
    "        mapes+=run_all_models(*fold)\n",
    "    mapes/=len(folds)\n",
    "    writer.writerow([\"fb_age_sex_normalized_data_used\", k, *mapes])\n",
    "    print(mapes)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Facebook age-sex corrected with autoregression"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "norm_columns = [col for col in fb_df.columns if 'normalized' in col]\n",
    "norm_columns.extend(['un_expat_total_age16_2017', 'un_expat_total_age16_2015'])\n",
    "splits = gen_splits(fb_df, norm_columns)\n",
    "\n",
    "for (k, folds) in splits.items():\n",
    "    print(k + \": \")\n",
    "    mapes = mapes = np.array([0.]*3)\n",
    "    for fold in folds: \n",
    "        mapes+=run_all_models(*fold)\n",
    "    mapes/=len(folds)\n",
    "    writer.writerow([\"autoregressive_with_fb_normalized\", k, *mapes])\n",
    "    print(mapes)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Facebook age-sex corrected with autoregression (2020)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "norm_columns = [col for col in fb_df.columns if 'normalized' in col]\n",
    "norm_columns.extend(['un_expat_total_age16_2017', 'un_expat_total_age16_2015'])\n",
    "splits = gen_splits(fb_df, norm_columns)\n",
    "\n",
    "for (k, folds) in splits.items():\n",
    "    print(k + \": \")\n",
    "    mapes = mapes = np.array([0.]*3)\n",
    "    for fold in folds: \n",
    "        mapes+=run_all_models(*fold)\n",
    "    mapes/=len(folds)\n",
    "    writer.writerow([\"autoregressive_with_fb_normalized\", k, *mapes])\n",
    "    print(mapes)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "norm_columns = [col for col in fb_df.columns if 'normalized' in col]\n",
    "norm_columns.extend(['un_expat_total_age16_2017', 'un_expat_total_age16_2015'])\n",
    "splits = gen_splits(fb_df, norm_columns, True)\n",
    "\n",
    "for (k, folds) in splits.items():\n",
    "    print(k + \": \")\n",
    "    mapes = mapes = np.array([0.]*3)\n",
    "    for fold in folds: \n",
    "        mapes+=run_all_models(*fold)\n",
    "    mapes/=len(folds)\n",
    "    writer.writerow([\"autoregressive_with_fb_normalized_data_used\", k, *mapes])\n",
    "    print(mapes)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All Predictors (2019)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "splits = gen_splits(fb_df)\n",
    "\n",
    "for (k, folds) in splits.items():\n",
    "    print(k + \": \")\n",
    "    mapes = mapes = np.array([0.]*3)\n",
    "    for fold in folds: \n",
    "        mapes+=run_all_models(*fold)\n",
    "    mapes/=len(folds)\n",
    "    #writer.writerow([\"all_preds\", k, *mapes])\n",
    "    print(mapes)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All Predictors (2020)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "splits = gen_splits(fb_df)\n",
    "\n",
    "for (k, folds) in splits.items():\n",
    "    print(k + \": \")\n",
    "    mapes = mapes = np.array([0.]*3)\n",
    "    for fold in folds: \n",
    "        mapes+=run_all_models(*fold)\n",
    "    mapes/=len(folds)\n",
    "    writer.writerow([\"all_preds\", k, *mapes])\n",
    "    print(mapes)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Age-Sex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_codes = list(set(fb_un_age_sex['country_code']))\n",
    "high_dev_isos = list(set(fb_un_age_sex.loc[fb_un_age_sex['un_development_lvl'] == 1, 'country_code']))\n",
    "low_dev_isos = list(set(fb_un_age_sex.loc[fb_un_age_sex['un_development_lvl'] == 0, 'country_code']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "randall_splits = [([country_codes[i] for i in train_split], [country_codes[i] for i in test_split]) for train_split, test_split in _iso_splits[\"all\"]]\n",
    "high_dev_splits = [([high_dev_isos[i] for i in train_split], [high_dev_isos[i] for i in test_split]) for train_split, test_split in _iso_splits[\"high\"]]\n",
    "low_dev_splits = [([low_dev_isos[i] for i in train_split], [low_dev_isos[i] for i in test_split]) for train_split, test_split in _iso_splits[\"low\"]]\n",
    "\n",
    "iso_splits = {\n",
    "            \"random_all\": randall_splits,\n",
    "            \"train_test_high\": high_dev_splits,\n",
    "            \"train_test_low\": low_dev_splits\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['age_sex_group_female_age0',\n",
    "       'age_sex_group_female_age1', 'age_sex_group_female_age10',\n",
    "       'age_sex_group_female_age2', 'age_sex_group_female_age3',\n",
    "       'age_sex_group_female_age4', 'age_sex_group_female_age5',\n",
    "       'age_sex_group_female_age6', 'age_sex_group_female_age7',\n",
    "       'age_sex_group_female_age8', 'age_sex_group_female_age9',\n",
    "       'age_sex_group_male_age0', 'age_sex_group_male_age1',\n",
    "       'age_sex_group_male_age10', 'age_sex_group_male_age2',\n",
    "       'age_sex_group_male_age3', 'age_sex_group_male_age4',\n",
    "       'age_sex_group_male_age5', 'age_sex_group_male_age6',\n",
    "       'age_sex_group_male_age7', 'age_sex_group_male_age8',\n",
    "       'age_sex_group_male_age9', 'migrant_pop_2015', 'migrant_pop_2017']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random_all : \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-de4e99eb84de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfb_un_age_sex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfb_un_age_sex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'country_code'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'migrant_pop_2019'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfb_un_age_sex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfb_un_age_sex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'country_code'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'migrant_pop_2019'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mmapes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_all_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0msubgroup_mapes\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0mmapes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-7391d68e77af>\u001b[0m in \u001b[0;36mrun_all_models\u001b[0;34m(X_train, X_test, y_train, y_test, preds)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# random forest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mrf_regr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomForestRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_depth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"mae\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mrf_regr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0mrf_y_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrf_regr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mrf_mape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMAPE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrf_y_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/cos597e/lib/python3.8/site-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    384\u001b[0m             \u001b[0;31m# parallel_backend contexts set at a higher level,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m             \u001b[0;31m# since correctness does not rely on using threads.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 386\u001b[0;31m             trees = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,\n\u001b[0m\u001b[1;32m    387\u001b[0m                              \u001b[0;34m**\u001b[0m\u001b[0m_joblib_parallel_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'threads'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m                 delayed(_parallel_build_trees)(\n",
      "\u001b[0;32m~/.conda/envs/cos597e/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1049\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1050\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/cos597e/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    864\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    865\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 866\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    867\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/cos597e/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    782\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    783\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 784\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    785\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/cos597e/lib/python3.8/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/cos597e/lib/python3.8/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/cos597e/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;31m# change the default number of processes to -1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m             return [func(*args, **kwargs)\n\u001b[0m\u001b[1;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/cos597e/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;31m# change the default number of processes to -1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m             return [func(*args, **kwargs)\n\u001b[0m\u001b[1;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/cos597e/lib/python3.8/site-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36m_parallel_build_trees\u001b[0;34m(tree, forest, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[1;32m    166\u001b[0m                                                         indices=indices)\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurr_sample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/cos597e/lib/python3.8/site-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m   1240\u001b[0m         \"\"\"\n\u001b[1;32m   1241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1242\u001b[0;31m         super().fit(\n\u001b[0m\u001b[1;32m   1243\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1244\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/cos597e/lib/python3.8/site-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    373\u001b[0m                                            min_impurity_split)\n\u001b[1;32m    374\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 375\u001b[0;31m         \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for k, folds in iso_splits.items():\n",
    "    subgroup_mapes = mapes = np.array([0.]*3)\n",
    "    total_mapes = mapes = np.array([0.]*3)\n",
    "    print(k, \": \")\n",
    "    for train, test in folds: \n",
    "        X_train = fb_un_age_sex.loc[fb_un_age_sex['country_code'].isin(train), columns]\n",
    "        X_test = fb_un_age_sex.loc[fb_un_age_sex['country_code'].isin(test), columns]\n",
    "        y_train = fb_un_age_sex.loc[fb_un_age_sex['country_code'].isin(train), 'migrant_pop_2019']\n",
    "        y_test = fb_un_age_sex.loc[fb_un_age_sex['country_code'].isin(test), 'migrant_pop_2019']\n",
    "        mapes, preds = run_all_models(X_train, X_test, y_train, y_test, preds=True)\n",
    "        subgroup_mapes+=mapes\n",
    "\n",
    "        y_test = fb_un_age_sex.loc[fb_un_age_sex['country_code'].isin(test), ['migrant_pop_2019', 'country_code']]\n",
    "        mapes_temp = []\n",
    "        for pred in preds:\n",
    "            pred = pd.DataFrame(pred, columns=['pop'])\n",
    "            pred['country_code'] = fb_un_age_sex.loc[fb_un_age_sex['country_code'].isin(test), 'country_code'].values\n",
    "            mapes_temp.append(MAPE(y_test.groupby('country_code').sum()['migrant_pop_2019'].values, pred.groupby('country_code').sum()['pop'].values))\n",
    "        total_mapes+=mapes_temp\n",
    "\n",
    "    subgroup_mapes = subgroup_mapes/len(folds)\n",
    "    total_mapes = total_mapes/len(folds)\n",
    "    writer.writerow([\"autoregressive_subgroups\", k, *subgroup_mapes])\n",
    "    writer.writerow([\"autoregressive_subgroups_added\", k, *total_mapes])\n",
    "    print(\"Subgroups: \", subgroup_mapes)\n",
    "    print(\"Total: \", total_mapes)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['age_sex_group_female_age0',\n",
    "       'age_sex_group_female_age1', 'age_sex_group_female_age10',\n",
    "       'age_sex_group_female_age2', 'age_sex_group_female_age3',\n",
    "       'age_sex_group_female_age4', 'age_sex_group_female_age5',\n",
    "       'age_sex_group_female_age6', 'age_sex_group_female_age7',\n",
    "       'age_sex_group_female_age8', 'age_sex_group_female_age9',\n",
    "       'age_sex_group_male_age0', 'age_sex_group_male_age1',\n",
    "       'age_sex_group_male_age10', 'age_sex_group_male_age2',\n",
    "       'age_sex_group_male_age3', 'age_sex_group_male_age4',\n",
    "       'age_sex_group_male_age5', 'age_sex_group_male_age6',\n",
    "       'age_sex_group_male_age7', 'age_sex_group_male_age8',\n",
    "       'age_sex_group_male_age9', 'migrant_pop_2015', 'migrant_pop_2017', 'fb_expats_normalized']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, folds in iso_splits.items():\n",
    "    subgroup_mapes = mapes = np.array([0.]*3)\n",
    "    total_mapes = mapes = np.array([0.]*3)\n",
    "    print(k, \": \")\n",
    "    for train, test in folds: \n",
    "        X_train = fb_un_age_sex.loc[fb_un_age_sex['country_code'].isin(train), columns]\n",
    "        X_test = fb_un_age_sex.loc[fb_un_age_sex['country_code'].isin(test), columns]\n",
    "        y_train = fb_un_age_sex.loc[fb_un_age_sex['country_code'].isin(train), 'migrant_pop_2019']\n",
    "        y_test = fb_un_age_sex.loc[fb_un_age_sex['country_code'].isin(test), 'migrant_pop_2019']\n",
    "        mapes, preds = run_all_models(X_train, X_test, y_train, y_test, preds=True)\n",
    "        subgroup_mapes+=mapes\n",
    "\n",
    "        y_test = fb_un_age_sex.loc[fb_un_age_sex['country_code'].isin(test), ['migrant_pop_2019', 'country_code']]\n",
    "        mapes_temp = []\n",
    "        for pred in preds:\n",
    "            pred = pd.DataFrame(pred, columns=['pop'])\n",
    "            pred['country_code'] = fb_un_age_sex.loc[fb_un_age_sex['country_code'].isin(test), 'country_code'].values\n",
    "            mapes_temp.append(MAPE(y_test.groupby('country_code').sum()['migrant_pop_2019'].values, pred.groupby('country_code').sum()['pop'].values))\n",
    "        total_mapes+=mapes_temp\n",
    "\n",
    "    subgroup_mapes = subgroup_mapes/len(folds)\n",
    "    total_mapes = total_mapes/len(folds)\n",
    "    writer.writerow([\"autoregressive_with_fb_normalized_subgroups\", k, *subgroup_mapes])\n",
    "    writer.writerow([\"autoregressive_with_fb_normalized_subgroups_added\", k, *total_mapes])\n",
    "    print(\"Subgroups: \", subgroup_mapes)\n",
    "    print(\"Total: \", total_mapes)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['age_sex_group_female_age0',\n",
    "       'age_sex_group_female_age1', 'age_sex_group_female_age10',\n",
    "       'age_sex_group_female_age2', 'age_sex_group_female_age3',\n",
    "       'age_sex_group_female_age4', 'age_sex_group_female_age5',\n",
    "       'age_sex_group_female_age6', 'age_sex_group_female_age7',\n",
    "       'age_sex_group_female_age8', 'age_sex_group_female_age9',\n",
    "       'age_sex_group_male_age0', 'age_sex_group_male_age1',\n",
    "       'age_sex_group_male_age10', 'age_sex_group_male_age2',\n",
    "       'age_sex_group_male_age3', 'age_sex_group_male_age4',\n",
    "       'age_sex_group_male_age5', 'age_sex_group_male_age6',\n",
    "       'age_sex_group_male_age7', 'age_sex_group_male_age8',\n",
    "       'age_sex_group_male_age9', 'fb_expats_normalized']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, folds in iso_splits.items():\n",
    "    subgroup_mapes = mapes = np.array([0.]*3)\n",
    "    total_mapes = mapes = np.array([0.]*3)\n",
    "    print(k, \": \")\n",
    "    for train, test in folds: \n",
    "        X_train = fb_un_age_sex.loc[fb_un_age_sex['country_code'].isin(train), columns]\n",
    "        X_test = fb_un_age_sex.loc[fb_un_age_sex['country_code'].isin(test), columns]\n",
    "        y_train = fb_un_age_sex.loc[fb_un_age_sex['country_code'].isin(train), 'migrant_pop_2019']\n",
    "        y_test = fb_un_age_sex.loc[fb_un_age_sex['country_code'].isin(test), 'migrant_pop_2019']\n",
    "        mapes, preds = run_all_models(X_train, X_test, y_train, y_test, preds=True)\n",
    "        subgroup_mapes+=mapes\n",
    "\n",
    "        y_test = fb_un_age_sex.loc[fb_un_age_sex['country_code'].isin(test), ['migrant_pop_2019', 'country_code']]\n",
    "        mapes_temp = []\n",
    "        for pred in preds:\n",
    "            pred = pd.DataFrame(pred, columns=['pop'])\n",
    "            pred['country_code'] = fb_un_age_sex.loc[fb_un_age_sex['country_code'].isin(test), 'country_code'].values\n",
    "            mapes_temp.append(MAPE(y_test.groupby('country_code').sum()['migrant_pop_2019'].values, pred.groupby('country_code').sum()['pop'].values))\n",
    "        total_mapes+=mapes_temp\n",
    "\n",
    "    subgroup_mapes = subgroup_mapes/len(folds)\n",
    "    total_mapes = total_mapes/len(folds)\n",
    "    writer.writerow([\"fb_normalized_subgroups\", k, *subgroup_mapes])\n",
    "    writer.writerow([\"fb_normalized_subgroups_added\", k, *total_mapes])\n",
    "    print(\"Subgroups: \", subgroup_mapes)\n",
    "    print(\"Total: \", total_mapes)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['age_sex_group_female_age0',\n",
    "       'age_sex_group_female_age1', 'age_sex_group_female_age10',\n",
    "       'age_sex_group_female_age2', 'age_sex_group_female_age3',\n",
    "       'age_sex_group_female_age4', 'age_sex_group_female_age5',\n",
    "       'age_sex_group_female_age6', 'age_sex_group_female_age7',\n",
    "       'age_sex_group_female_age8', 'age_sex_group_female_age9',\n",
    "       'age_sex_group_male_age0', 'age_sex_group_male_age1',\n",
    "       'age_sex_group_male_age10', 'age_sex_group_male_age2',\n",
    "       'age_sex_group_male_age3', 'age_sex_group_male_age4',\n",
    "       'age_sex_group_male_age5', 'age_sex_group_male_age6',\n",
    "       'age_sex_group_male_age7', 'age_sex_group_male_age8',\n",
    "       'age_sex_group_male_age9', 'fb_expats_normalized']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = iso_splits[\"random_all\"]\n",
    "\n",
    "mapes = 0\n",
    "for train, test in folds:\n",
    "    X_train = fb_un_age_sex.loc[fb_un_age_sex['country_code'].isin(train), columns]\n",
    "    X_train['fb_expats_normalized'] = np.log(X_train['fb_expats_normalized'])\n",
    "    X_test = fb_un_age_sex.loc[fb_un_age_sex['country_code'].isin(test), columns]\n",
    "    X_test['fb_expats_normalized'] = np.log(X_test['fb_expats_normalized'])\n",
    "    y_train = fb_un_age_sex.loc[fb_un_age_sex['country_code'].isin(train), 'migrant_pop_2019']\n",
    "    y_train = np.log(y_train)\n",
    "    y_test = fb_un_age_sex.loc[fb_un_age_sex['country_code'].isin(test), 'migrant_pop_2019']\n",
    "\n",
    "    lin_reg = LinearRegression()\n",
    "    lin_reg.fit(X_train, y_train)\n",
    "    lin_y_pred = lin_reg.predict(X_test)\n",
    "    \n",
    "    mapes+=MAPE(y_test, np.exp(lin_y_pred))\n",
    "mapes/len(folds)\n",
    "X_train[:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
